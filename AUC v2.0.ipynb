{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5a61a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e7fbfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.7.1-py3-none-win_amd64.whl (89.1 MB)\n",
      "     ---------------------------------------- 89.1/89.1 MB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy in c:\\users\\sermi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from xgboost) (1.5.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\sermi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from xgboost) (1.22.4)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.7.1\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b4207a",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb902ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_feats = pd.read_csv('players_feats.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7be569",
   "metadata": {},
   "source": [
    "#### Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "041b3c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_merge(a, b):\n",
    "\n",
    "    c = pd.merge(a, b, how='left', left_on=['team1_id', 'map_id'],right_on=['team_id','map_id'])\n",
    "    c = pd.merge(c, b, how='left', left_on=['team2_id', 'map_id'],right_on=['team_id','map_id'])\n",
    "    c = c.drop(['map_name_x', 'map_name_y', 'team_id_x', 'team_id_y', 'map_id'], axis=1)    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403d98a6",
   "metadata": {},
   "source": [
    "## Dataset of team statistic"
   ]
  },
  {
   "cell_type": "raw",
   "id": "623b9d66",
   "metadata": {},
   "source": [
    "Создаем датасет, объединяющий результаты игроков в показатели всей команды. Усредняем на количество сыграных карт те параметры, которые нуждаются в этом, для сравнения двух команд между собой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51543858",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_feats = pd.DataFrame(columns = [\n",
    "    'team_id',\n",
    "    'map_name',\n",
    "    'map_id',\n",
    "    'total_kills',\n",
    "    'headshots',\n",
    "    'total_deaths',\n",
    "    'damage_per_round',\n",
    "    'grenade_damage_per_round',\n",
    "    'maps_played',\n",
    "    'rounds_played',\n",
    "    'kills_per_round',\n",
    "    'assists_per_round',\n",
    "    'deaths_per_round',\n",
    "    'kill_death_difference',\n",
    "    'total_opening_kills',\n",
    "    'total_opening_deaths',\n",
    "    'team_win_percent_after_first_kill',\n",
    "    'first_kill_in_won_rounds'])\n",
    "\n",
    "\n",
    "team_feats['team_id'] = players_feats['team_id']\n",
    "team_feats['map_name'] = players_feats['map_name']\n",
    "team_feats['map_id'] = players_feats['map_id']\n",
    "\n",
    "for i in team_feats.columns[3:]:\n",
    "    s = 0\n",
    "    for j in range(5):\n",
    "        s += players_feats['p'+str(j+1)+'_'+i]\n",
    "    if i in ['maps_played', 'team_win_percent_after_first_kill', 'rounds_played']:\n",
    "        s = s/5\n",
    "    team_feats[i] = s\n",
    "    \n",
    "for i in [\n",
    "    'total_kills',\n",
    "    'headshots',\n",
    "    'total_deaths',\n",
    "    'rounds_played',\n",
    "    'kill_death_difference',\n",
    "    'total_opening_kills',\n",
    "    'total_opening_deaths',\n",
    "    'first_kill_in_won_rounds']:\n",
    "    team_feats[i] /= team_feats.maps_played"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7c2b416",
   "metadata": {},
   "source": [
    "Отбор фичей, которые. по результатам тестов, больше всего влияют на точность "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "259d4c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'team_id',\n",
    "    'map_name',\n",
    "    'map_id',\n",
    "    'total_kills',\n",
    "#     'damage_per_round',\n",
    "    'maps_played',\n",
    "#     'kills_per_round',\n",
    "    'kill_death_difference',\n",
    "    'total_opening_kills',\n",
    "    'total_opening_deaths',\n",
    "    'team_win_percent_after_first_kill',\n",
    "#     'first_kill_in_won_rounds'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "393f799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_teams_v2 = train_merge(train, team_feats.loc[:, features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52f2034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_name_list = np.unique(train_with_teams_v2.map_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519a2be6",
   "metadata": {},
   "source": [
    "## Dataset of team difference statistic"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bca5e137",
   "metadata": {},
   "source": [
    "Создаем датасет, включающий разницу между показателями двух играющих команд. \n",
    "Основным фактором всегда является сила команд, поэтому будем отталкиваться от нее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cf24e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int((len(train_with_teams_v2.columns) - 4) / 2)\n",
    "\n",
    "train_team_diff = train_with_teams_v2.iloc[:, :3].copy()\n",
    "train_team_diff = pd.concat([train_team_diff, train_with_teams_v2.iloc[:, len(train_with_teams_v2.columns)-n:].copy()], axis=1)\n",
    "train_team_diff = pd.concat([train_team_diff, train_with_teams_v2.map_name], axis=1)\n",
    "\n",
    "for i in range(n):\n",
    "    train_team_diff.iloc[:, i+3] -= train_with_teams_v2.iloc[:, i+3]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c42d97da",
   "metadata": {},
   "source": [
    "Разделим датасет по картем, ведь для каждой карты могут быть свои основные характеристики. Итоговые обучающие датасеты, в нашем случае, получаются небольшими, однако, по моему мнению, это более правильный подход для работы с глобальными данными.\n",
    "\n",
    "Для каждой карты выберем лучшую из N моделей, чтобы в дальнейшем использовать ее для предикта тестовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56948f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ancient\n",
      "Best AUC score = 0.8571428571428572\n",
      "Dust2\n",
      "Best AUC score = 0.7321428571428571\n",
      "Inferno\n",
      "Best AUC score = 0.7777777777777778\n",
      "Mirage\n",
      "Best AUC score = 0.7357142857142858\n",
      "Nuke\n",
      "Best AUC score = 0.7735042735042734\n",
      "Overpass\n",
      "Best AUC score = 0.8181818181818181\n",
      "Vertigo\n",
      "Best AUC score = 0.8125\n"
     ]
    }
   ],
   "source": [
    "N = 40\n",
    "\n",
    "maps_models = {}\n",
    "\n",
    "for map_name in map_name_list:\n",
    "    print(map_name)\n",
    "    maps_models[map_name] = [None, 0]\n",
    "    for j in range(N):\n",
    "        y = train_team_diff[train_team_diff.map_name==map_name].who_win.copy()\n",
    "        X = train_team_diff[train_team_diff.map_name==map_name].drop(['who_win', 'team1_id', 'team2_id', 'map_name'], axis=1)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "        X = scaler.transform(X)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "        model = xgb.XGBRFClassifier(\n",
    "                                    objective = \"binary:logistic\",\n",
    "                                    colsample_bynode=1,\n",
    "                                    learning_rate=0.01,\n",
    "                                    max_depth=3,\n",
    "                                    n_estimators=500,\n",
    "                                    reg_lambda=1,\n",
    "                                   )\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        predictions = model.predict(X_test)\n",
    "\n",
    "        fpr, tpr, treshholds = roc_curve(y_test, predictions)\n",
    "        auc = roc_auc_score(y_test, predictions)\n",
    "\n",
    "        # При таком подходе в ходе экспериментов некоторые из моделей показывали результат предсказаний ниже 50%. \n",
    "        # Однако в нашем случае бинарного результата, достаточно отразить данные, чтобы вернуть предсказания к нормальным результатам.\n",
    "        \n",
    "        if auc<0.5:\n",
    "            model.fit(X_train, abs(y_train-1))\n",
    "            predictions = model.predict(X_test)\n",
    "            fpr, tpr, treshholds = roc_curve(y_test, predictions)\n",
    "            auc = roc_auc_score(y_test, predictions)\n",
    "        \n",
    "        if maps_models[map_name][1] < auc:\n",
    "            maps_models[map_name] = [model, auc]\n",
    "    \n",
    "\n",
    "    print('Best AUC score =', maps_models[map_name][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a2a8c0",
   "metadata": {},
   "source": [
    "## Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "694547b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = train_merge(test, team_feats.loc[:, features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a67b639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int((len(train_test.columns) - 4) / 2)\n",
    "\n",
    "test_team_diff = train_test.iloc[:, :3].copy()\n",
    "test_team_diff = pd.concat([test_team_diff, train_test.iloc[:, len(train_test.columns)-n:].copy()], axis=1)\n",
    "test_team_diff = pd.concat([test_team_diff, train_test.map_name], axis=1)\n",
    "\n",
    "for i in range(n):\n",
    "    test_team_diff.iloc[:, i+3] -= train_test.iloc[:, i+3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ca3ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = np.array([0 for i in range(len(test))])\n",
    "\n",
    "for map_name in map_name_list:\n",
    "    X = test_team_diff[test_team_diff.map_name==map_name].drop(['index','team1_id', 'team2_id', 'map_name'], axis=1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "\n",
    "    model = maps_models[map_name][0]\n",
    "    predictions = model.predict(X)\n",
    "    test_predict[test_team_diff.map_name==map_name] = predictions\n",
    "\n",
    "test_to_save = test.copy()\n",
    "test_to_save['predictions'] = test_predict\n",
    "test_to_save.to_csv('test_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c668e80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
